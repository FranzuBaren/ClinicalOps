\documentclass[11pt,a4paper]{article}

% ═══════════════════════════════════════════════════════════════════════
% JCDE-STYLE LAYOUT
% ═══════════════════════════════════════════════════════════════════════
\usepackage[margin=2.5cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{mathptmx}          % Times Roman font (text + math) — matches JCDE
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage[colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{fancyhdr}
\usepackage{enumitem}
\usepackage{authblk}
\usepackage{abstract}
\usepackage{tcolorbox}
\tcbuselibrary{skins,breakable}
\usepackage{setspace}

% ── Spacing (JCDE-like: single-spaced, modest parskip) ──────────────
\setlength{\parskip}{4pt plus 1pt}
\setlength{\parindent}{0pt}
\setlength{\abovecaptionskip}{6pt}
\setlength{\belowcaptionskip}{3pt}
\setlist{nosep,leftmargin=1.5em}

% ── Section numbering (JCDE style: "1. Introduction") ───────────────
\renewcommand{\thesection}{\arabic{section}.}
\renewcommand{\thesubsection}{\arabic{section}.\arabic{subsection}}
\renewcommand{\thesubsubsection}{\arabic{section}.\arabic{subsection}.\arabic{subsubsection}}

% ── Header/Footer ───────────────────────────────────────────────────
\pagestyle{fancy}\fancyhf{}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0pt}

% ── Box environments ────────────────────────────────────────────────
\definecolor{lightblue}{HTML}{E8F0FE}
\definecolor{lightgreen}{HTML}{E8F5E9}
\definecolor{lightred}{HTML}{FDECEC}
\definecolor{lightamber}{HTML}{FFF3E0}
\definecolor{mathbg}{HTML}{F8F6FF}
\definecolor{boxgray}{HTML}{F5F5F5}

\newtcolorbox{scenariobox}[1][]{enhanced,breakable,colback=lightblue,colframe=blue!50!black,boxrule=0pt,leftrule=3pt,arc=1pt,fonttitle=\bfseries\small,title={#1},top=4pt,bottom=4pt,left=8pt,right=8pt}
\newtcolorbox{insightbox}[1][]{enhanced,breakable,colback=lightgreen,colframe=green!50!black,boxrule=0pt,leftrule=3pt,arc=1pt,fonttitle=\bfseries\small,title={#1},top=4pt,bottom=4pt,left=8pt,right=8pt}
\newtcolorbox{warningbox}[1][]{enhanced,breakable,colback=lightred,colframe=red!50!black,boxrule=0pt,leftrule=3pt,arc=1pt,fonttitle=\bfseries\small,title={#1},top=4pt,bottom=4pt,left=8pt,right=8pt}
\newtcolorbox{techbox}[1][]{enhanced,breakable,colback=lightamber,colframe=orange!50!black,boxrule=0pt,leftrule=3pt,arc=1pt,fonttitle=\bfseries\small,title={#1},top=4pt,bottom=4pt,left=8pt,right=8pt}
\newtcolorbox{mathbox}[1][]{enhanced,breakable,colback=mathbg,colframe=blue!30,boxrule=0.5pt,arc=2pt,fonttitle=\bfseries\small,title={#1},top=5pt,bottom=5pt,left=8pt,right=8pt}
\newtcolorbox{graybox}{enhanced,colback=boxgray,colframe=boxgray,boxrule=0pt,arc=3pt,top=6pt,bottom=6pt,left=10pt,right=10pt}

% ── Math shortcuts ──────────────────────────────────────────────────
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\KL}{\mathrm{KL}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\bm}{\mathbf{m}}
\newcommand{\balpha}{\boldsymbol{\alpha}}

% ═══════════════════════════════════════════════════════════════════════
\begin{document}
% ═══════════════════════════════════════════════════════════════════════

% ── TITLE BLOCK (JCDE style) ────────────────────────────────────────
\title{\textbf{ClinOps AI: Deep Learning for Clinical Trial Safety Intelligence}}

\author{Francesco Orsi\thanks{Corresponding Author. E-mail: francesco.orsi84@gmail.com}}

\date{February 17, 2026}
\maketitle
\thispagestyle{fancy}

% ── ABSTRACT (JCDE style: bold "Abstract" label) ────────────────────
\begin{abstract}
\noindent We present ClinOps AI, an end-to-end analytical pipeline that applies modern deep learning to clinical trial safety monitoring using real FDA-standard CDISC SDTM data. Working with the PHUSE CDISCPILOT01 dataset (N=254, Xanomeline vs Placebo, Alzheimer's disease), we formalize and implement a six-stage pipeline: pure-Python SAS Transport parsing, Pydantic v2 data validation, DuckDB analytical queries, nine publication-quality clinical visualizations, a deep learning suite comprising conditional variational autoencoder augmentation, self-supervised masked feature pretraining, and MLP classification with Monte Carlo Dropout uncertainty, and interpretable feature attribution via Integrated Gradients. An 8-model ablation study under 5-fold stratified cross-validation demonstrates that self-supervised pretraining contributes $\Delta\text{AUROC} = +0.067$ and generative augmentation adds $+0.021$ incrementally. However, $\ell_2$-regularized logistic regression (AUROC=0.690) remains competitive at this sample size. We identify and correct a critical data leakage issue, provide honest assessment of the extreme class imbalance (3/254 SAE subjects), and demonstrate that neural interpretability tools --- specifically attention weights and Integrated Gradients --- reveal clinically novel multivariate temporal patterns invisible to traditional pharmacovigilance.
\end{abstract}

\noindent\textbf{Keywords:} clinical trials, pharmacovigilance, deep learning, CDISC SDTM, adverse events, interpretable ML.

% ══════════════════════════════════════════════════════════════════════
\section{Introduction}
% ══════════════════════════════════════════════════════════════════════

Clinical trials generate safety data at unprecedented scale. A single Phase III oncology trial may enroll 3,000+ subjects across 200 sites in 30 countries, producing tens of thousands of adverse event (AE) records, millions of laboratory measurements, and increasingly continuous vital sign streams from wearable devices. Yet the analytical toolkit applied to this data has not fundamentally changed since the ICH E9 guideline was finalized in 1998.

Medical monitors, Data Safety Monitoring Boards (DSMBs), and regulatory reviewers still rely on three primary instruments: frequency tables (``25\% on drug vs 8\% on placebo reported nausea'' --- no temporal dimension, no multivariate structure), patient listings (individual narratives sorted by seriousness, which do not scale beyond dozens of subjects), and pre-specified Tables, Listings, and Figures (TLFs) locked into statistical analysis plans months before unblinding, precluding any adaptive or exploratory analysis.

These methods are regulatory-compliant and well-understood. However, they leave four categories of safety questions systematically unanswered:

\begin{scenariobox}[Unresolved Safety Questions in Current Pharmacovigilance]
\begin{enumerate}
\item \textbf{Temporal clustering.} When do adverse events concentrate within the study timeline? Frequency tables aggregate across the entire study period, obscuring acute onset patterns, cumulative dose effects, and organ-specific latency differences.
\item \textbf{Multivariate signals.} Pruritus from a transdermal patch is expected. But does pruritus \textit{combined with} dizziness \textit{combined with} weight decrease in the same patient predict a serious event? Univariate disproportionality analyses cannot detect feature interactions.
\item \textbf{Individual risk prediction.} A subject has experienced 5 mild adverse events in 6 weeks. Is this a harbinger of a serious adverse event, or simply a high-reporter phenotype? Without predictive modeling, the medical monitor relies on intuition.
\item \textbf{Per-patient explainability.} If a safety concern is escalated to the DSMB, mechanistic reasoning is required --- not a global performance metric, but an explanation of \textit{why this patient} is flagged \textit{based on which features}.
\end{enumerate}
\end{scenariobox}

Modern deep learning can address all four --- temporal modeling via recurrent architectures, multivariate pattern detection via representation learning, individual-level prediction via subject-level classifiers, and feature attribution via gradient-based interpretability methods. The challenge is applying these methods with the rigor that clinical data demands: proper validation protocols, systematic leakage prevention, honest uncertainty quantification, and transparent limitation reporting.

This paper presents ClinOps AI, a complete analytical pipeline that demonstrates what modern ML can --- and cannot --- achieve on real clinical trial safety data.

% ══════════════════════════════════════════════════════════════════════
\section{Data: PHUSE CDISC Pilot Study}
% ══════════════════════════════════════════════════════════════════════

\subsection{Source, Provenance, and Licensing}

This project uses the \textbf{PHUSE CDISC Pilot Study (CDISCPILOT01)}, a publicly available clinical trial dataset maintained by the Pharmaceutical Users Software Exchange (PHUSE). The study evaluated Xanomeline transdermal system at two dose levels (54\,mg/day and 81\,mg/day) versus Placebo in 254 subjects with mild-to-moderate Alzheimer's disease. The data was originally submitted to the U.S. Food and Drug Administration as part of a CDISC pilot program demonstrating the feasibility of the Study Data Tabulation Model (SDTM) for electronic regulatory submissions.

\begin{techbox}[Data Access, Licensing, and Privacy Statement]
\textbf{Repository:} \url{https://github.com/phuse-org/phuse-scripts/tree/master/data/sdtm/cdiscpilot01}

\textbf{Format:} SAS Transport v5 (XPT), the FDA-mandated electronic submission format per 21 CFR Part 11.

\textbf{License:} Publicly available under the PHUSE open-source initiative for educational and research purposes. No patient-level consent restrictions apply, as the data has been fully anonymized and released for public use.

\textbf{Privacy:} All subject identifiers are synthetic study-assigned codes (e.g., ``01-701-1015''). No names, dates of birth, geographic identifiers, or other HIPAA-defined Protected Health Information (PHI) are present. The data cannot be linked to real individuals.

\textbf{Limitations of use:} This is a pilot/demonstration dataset. While it follows FDA submission standards and contains real clinical measurements, it should not be used to draw clinical conclusions about Xanomeline's actual safety or efficacy. All analyses in this paper are methodological demonstrations, not clinical evaluations.
\end{techbox}

\subsection{SDTM Domain Structure}

The dataset comprises six SDTM domains, each stored as a separate XPT binary file:

\begin{table}[H]
\centering\small
\caption{SDTM domains used in this project.}
\label{tab:domains}
\begin{tabular}{llrl}
\toprule
\textbf{Domain} & \textbf{Description} & \textbf{Records} & \textbf{Key Variables} \\
\midrule
DM & Demographics & 254 & AGE, SEX, RACE, ARM, RFSTDTC \\
AE & Adverse Events & 1,191 & AEDECOD, AEBODSYS, AESEV, AESER, AESTDY \\
EX & Exposure & 591 & EXDOSE, EXSTDTC, EXENDTC, EXROUTE \\
VS & Vital Signs & 6,208 & VSTESTCD, VSSTRESN, VISITNUM \\
LB & Laboratory & 13,988 & LBTESTCD, LBSTRESN, LBNRIND \\
DS & Disposition & 254 & DSDECOD, DSTERM, DSSTDTC \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Study Design and Population}

\begin{table}[H]
\centering\small
\caption{Study arms and baseline characteristics.}
\label{tab:arms}
\begin{tabular}{lccc}
\toprule
& \textbf{Placebo} & \textbf{Xano Low (54\,mg)} & \textbf{Xano High (81\,mg)} \\
\midrule
$N$ enrolled & 86 & 84 & 84 \\
Mean age (yr) & $\sim$75 & $\sim$75 & $\sim$75 \\
\% Female & $\sim$53 & $\sim$50 & $\sim$55 \\
Total AE records & 202 & 456 & 533 \\
SAE subjects & 0 & 1 & 2 \\
\bottomrule
\end{tabular}
\end{table}

A critical constraint: only $N_1 = 3$ subjects out of $N = 254$ (1.2\%) experienced serious adverse events. This extreme class imbalance ratio of $N_0/N_1 \approx 84$ is the central challenge for any predictive modeling effort on this dataset and motivates both the generative augmentation and the honest limitation reporting throughout this work.

% ══════════════════════════════════════════════════════════════════════
\section{Pipeline Architecture}
% ══════════════════════════════════════════════════════════════════════

The system comprises six stages, each with a well-defined input, transformation, and output:

\begin{table}[H]
\centering\small
\caption{End-to-end pipeline: six stages from raw binary files to clinical decisions.}
\label{tab:pipeline}
\begin{tabular}{clll}
\toprule
\textbf{\#} & \textbf{Stage} & \textbf{Technology} & \textbf{Output} \\
\midrule
1 & XPT parsing & Pure-Python (\texttt{struct}, \texttt{BytesIO}) & Polars DataFrames \\
2 & Validation & Pydantic v2 SDTM models & Conformance report \\
3 & SQL analytics & DuckDB (zero-copy on Polars) & Clinical summary tables \\
4 & Visualization & matplotlib, seaborn, scipy & 9 figures (PDF/PNG, 300 DPI) \\
5 & Modeling & PyTorch (CVAE, MLP, GRU) & 8-model ablation with CI \\
6 & Interpretability & Integrated Gradients, attention & Per-subject explanations \\
\bottomrule
\end{tabular}
\end{table}

Stages 1--3 handle data ingestion. The XPT parser decodes IBM 360 floating-point representation and 80-byte fixed-width header records in approximately 60 lines of Python, eliminating the \$15,000/year SAS license dependency. Pydantic v2 models enforce CDISC business rules at parse time (e.g., if $\texttt{AESER} = \text{`Y'}$, then $\texttt{AEOUT} \neq \texttt{NULL}$). DuckDB provides zero-copy SQL execution directly on Polars DataFrames.

% ══════════════════════════════════════════════════════════════════════
\section{Statistical Methods for Clinical Analytics}
% ══════════════════════════════════════════════════════════════════════

\subsection{Disproportionality Analysis (Volcano Plot)}

For each AE preferred term $j \in \{1, \ldots, J\}$, we construct the $2 \times 2$ contingency table comparing the High Dose arm to Placebo and test the null hypothesis $H_0: \pi_j^{\text{drug}} = \pi_j^{\text{placebo}}$ using Fisher's exact test.

\begin{mathbox}[Definition 1: Fisher's Exact Test for AE Disproportionality]
Let $a_j, b_j, c_j, d_j$ denote the cell counts:
\begin{equation}
\begin{pmatrix} a_j & b_j \\ c_j & d_j \end{pmatrix} = \begin{pmatrix} \text{Drug} \cap \text{AE}_j & \text{Placebo} \cap \text{AE}_j \\ \text{Drug} \cap \overline{\text{AE}_j} & \text{Placebo} \cap \overline{\text{AE}_j} \end{pmatrix}
\end{equation}
The exact $p$-value under the hypergeometric distribution is:
\begin{equation}
p_j = \sum_{k \geq a_j} \frac{\binom{a_j + b_j}{k}\,\binom{c_j + d_j}{n_1 - k}}{\binom{N}{n_1}}
\end{equation}
where $n_1 = a_j + c_j$ is the drug arm total and $N = a_j + b_j + c_j + d_j$.

The relative risk and its log-transform for the volcano $x$-axis:
\begin{equation}
\text{RR}_j = \frac{a_j / (a_j + c_j)}{b_j / (b_j + d_j)}, \qquad x_j = \log_2(\text{RR}_j)
\end{equation}
The $y$-axis transforms the $p$-value to a significance scale: $y_j = -\log_{10}(p_j)$.

\textbf{Signal detection rule:} Term $j$ is flagged if $y_j > -\log_{10}(0.05) \approx 1.30$ and $|x_j| > 0.5$.
\end{mathbox}

\subsection{Survival Analysis (Kaplan--Meier Estimator)}

Let $T_i$ be the time-to-first-AE for subject $i$, with potential right-censoring at the end of follow-up. The Kaplan--Meier estimator of the survival function $S(t) = \Pr(T > t)$ is:

\begin{mathbox}[Definition 2: Kaplan--Meier Estimation with Greenwood Variance]
\begin{equation}
\hat{S}(t) = \prod_{t_k \leq t}\left(1 - \frac{d_k}{n_k}\right)
\end{equation}
where $d_k$ is the number of events at ordered event time $t_k$ and $n_k$ is the number at risk just prior to $t_k$. The Greenwood formula gives the variance estimator:
\begin{equation}
\widehat{\mathrm{Var}}\!\left[\hat{S}(t)\right] = \hat{S}(t)^2 \sum_{t_k \leq t}\frac{d_k}{n_k(n_k - d_k)}
\end{equation}
Pointwise 95\% confidence bands: $\hat{S}(t) \pm z_{0.975}\sqrt{\widehat{\mathrm{Var}}[\hat{S}(t)]}$ where $z_{0.975} = 1.96$.
\end{mathbox}

\subsection{Severity Profile Analysis}

For each treatment arm $a \in \{\text{Placebo}, \text{Low}, \text{High}\}$, the severity distribution is modeled as a multinomial:
\begin{equation}
(n_{\text{mild}}^a, n_{\text{moderate}}^a, n_{\text{severe}}^a) \sim \text{Multinomial}\!\left(N^a;\; \pi_{\text{mild}}^a, \pi_{\text{moderate}}^a, \pi_{\text{severe}}^a\right)
\end{equation}
with maximum likelihood estimates $\hat{\pi}_g^a = n_g^a / N^a$. The dose-dependent severity shift is assessed visually via stacked bar charts (Figure~\ref{fig:dashboard}B).

\subsection{Exposure--Response Correlation}

The linear relationship between cumulative dose $D_i = \sum_{k} \text{EXDOSE}_{ik}$ and total AE count $Y_i$ is quantified by Pearson's product-moment correlation:
\begin{mathbox}[Definition 3: Pearson Correlation with $t$-Test]
\begin{equation}
r = \frac{\sum_{i=1}^{N}(D_i - \bar{D})(Y_i - \bar{Y})}{\sqrt{\sum_{i}(D_i - \bar{D})^2\;\sum_{i}(Y_i - \bar{Y})^2}}
\end{equation}
Under $H_0: \rho = 0$, the test statistic $t = r\sqrt{(N-2)/(1-r^2)}$ follows a $t$-distribution with $N-2$ degrees of freedom:
\begin{equation}
p = 2\;\Pr\!\left(|T| \geq |t|\right), \qquad T \sim t_{N-2}
\end{equation}
Our data yields $r = 0.93$, $p < 0.001$ --- a remarkably strong linear relationship.
\end{mathbox}

% ══════════════════════════════════════════════════════════════════════
\section{Deep Learning: Mathematical Formulation}
% ══════════════════════════════════════════════════════════════════════

\subsection{Feature Space Construction}

Each subject $i$ is represented by a feature vector $\bx_i \in \R^d$ with $d = 14$ dimensions drawn from four clinical domains, and a binary label $y_i \in \{0, 1\}$ indicating SAE occurrence. All features are standardized to zero mean and unit variance: $\tilde{x}_{ij} = (x_{ij} - \hat{\mu}_j)/\hat{\sigma}_j$.

\begin{mathbox}[Definition 4: Feature Vector (Leakage-Free)]
\begin{align}
\bx_i = \big[&\underbrace{\text{AGE}_i,\;\;\mathbb{1}[\text{female}_i],\;\;\mathbb{1}[\text{placebo}_i],\;\;\mathbb{1}[\text{high\_dose}_i]}_{\text{Demographics (4 features)}}, \notag\\[4pt]
&\underbrace{\textstyle\sum_k \text{EXDOSE}_{ik},\;\;\overline{\text{EXDOSE}}_i,\;\;|\{k : \text{EX}_{ik}\}|}_{\text{Exposure (3 features)}}, \notag\\[4pt]
&\underbrace{|\{j : \text{AESER}_{ij} \neq \text{Y}\}|,\;\;|\text{unique}(\text{AEDECOD}_i)|,\;\;\min_j \text{AESTDY}_{ij},\;\;\text{sd}(\text{AESTDY}_{ij})}_{\text{Non-serious AE patterns (4 features)}}, \notag\\[4pt]
&\underbrace{\overline{\text{SYSBP}}_i^{\,(1{:}3)},\;\;\overline{\text{DIABP}}_i^{\,(1{:}3)},\;\;\overline{\text{PULSE}}_i^{\,(1{:}3)}}_{\text{Baseline vitals (3 features)}}\;\big] \label{eq:features}
\end{align}
Target: $y_i = \mathbb{1}\!\left[\exists\, j : \text{AESER}_{ij} = \text{`Y'}\right]$, computed independently from the raw AE domain.
\end{mathbox}

\begin{warningbox}[Data Leakage: Identified and Corrected]
An initial implementation included $\texttt{n\_sae}_i = \sum_j \mathbb{1}[\text{AESER}_{ij} = \text{Y}]$ as a feature, while using $y_i = \mathbb{1}[\texttt{n\_sae}_i > 0]$ as the target. This created a perfect tautology: $\text{AUROC} \approx 1.000$ for all models. The corrected version: (1) excludes all SAE-derived quantities from $\bx_i$; (2) replaces total AE count with non-serious AE count; (3) computes $y_i$ independently by querying the AE domain.
\end{warningbox}

\subsection{Conditional Variational Autoencoder (CVAE)}

With $N_1 = 3$ and $N_0 = 251$, class imbalance is extreme ($N_0/N_1 \approx 84$). SMOTE generates synthetic points by linear interpolation $\bx^* = \bx_i + \lambda(\bx_j - \bx_i)$, $\lambda \sim \text{Uniform}(0,1)$, which assumes local linearity of the data manifold. Instead, we learn the full conditional distribution:

\begin{mathbox}[Definition 5: CVAE with ELBO Objective]
\textbf{Encoder} (variational posterior):
\begin{equation}
q_\phi(\bz | \bx, y) = \mathcal{N}\!\left(\bz;\;\mu_\phi(\bx, y),\;\text{diag}(\sigma_\phi^2(\bx, y))\right)
\end{equation}
where $\mu_\phi, \log\sigma_\phi^2: \R^{d+1} \to \R^{8}$ are parameterized by a neural network with layers: Linear$(d{+}1, 64) \to$ BatchNorm $\to$ LeakyReLU$(0.2) \to$ Linear$(64, 32) \to$ LeakyReLU$(0.2)$, followed by two linear heads for $\mu$ and $\log\sigma^2$.

\textbf{Decoder} (likelihood):
\begin{equation}
p_\theta(\bx | \bz, y) = \mathcal{N}\!\left(\bx;\; f_\theta(\bz, y),\; \sigma^2_{\text{dec}} I\right)
\end{equation}

\textbf{Training objective} (maximize the Evidence Lower Bound):
\begin{equation}
\mathcal{L}_{\text{ELBO}}(\theta,\phi;\bx_i,y_i) = \underbrace{\E_{q_\phi(\bz|\bx_i,y_i)}\!\left[\log p_\theta(\bx_i|\bz,y_i)\right]}_{\text{Reconstruction accuracy}} - \underbrace{\beta \cdot D_{\KL}\!\left(q_\phi(\bz|\bx_i,y_i) \;\|\; p(\bz)\right)}_{\text{Latent regularization}}
\end{equation}
with prior $p(\bz) = \mathcal{N}(\mathbf{0}, I)$ and $\beta = 0.3$.

\textbf{Closed-form KL divergence} (diagonal Gaussian vs standard normal):
\begin{equation}
D_{\KL}(q\|p) = -\frac{1}{2}\sum_{k=1}^{8}\left(1 + \log\sigma_k^2 - \mu_k^2 - \sigma_k^2\right) \label{eq:kl}
\end{equation}

\textbf{Reparameterization trick} (enables backpropagation through sampling):
\begin{equation}
\bz = \mu_\phi(\bx, y) + \sigma_\phi(\bx, y) \odot \boldsymbol{\epsilon}, \qquad \boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, I) \label{eq:reparam}
\end{equation}

\textbf{Synthetic minority generation:} Sample $\bz^* \sim \mathcal{N}(\mathbf{0}, I)$ and decode: $\bx^* = f_\theta(\bz^*, y{=}1)$.
\end{mathbox}

Training: 400 epochs, Adam optimizer ($\eta = 10^{-3}$, weight decay $10^{-5}$), full-batch on $N = 254$ samples.

\subsection{Self-Supervised Pretraining: Masked Feature Autoencoder}

Following the masked language modeling paradigm of BERT and the masked autoencoder approach of He et al. (2022), we pretrain a feature encoder $f_\psi: \R^d \to \R^{32}$ without using any labels:

\begin{mathbox}[Definition 6: Masked Feature Autoencoder]
\textbf{Masking strategy:} For each training sample $\bx_i$, generate a binary mask $\bm_i \sim \text{Bernoulli}(p{=}0.3)^d$ and zero out the masked features:
\begin{equation}
\tilde{\bx}_i = \bx_i \odot (\mathbf{1} - \bm_i) \label{eq:mask}
\end{equation}

\textbf{Architecture:} Encoder $f_\psi$: Linear$(d, 64) \to$ BatchNorm $\to$ GELU $\to$ Dropout$(0.1) \to$ Linear$(64, 32) \to$ BatchNorm $\to$ GELU. Decoder $g_\psi$: Linear$(32, 64) \to$ GELU $\to$ Linear$(64, d)$.

\textbf{Objective:} Reconstruct only the masked positions (not the visible ones):
\begin{equation}
\mathcal{L}_{\text{MAE}}(\psi) = \frac{1}{|\mathcal{M}_i|}\sum_{j \in \mathcal{M}_i}\left(x_{ij} - \hat{x}_{ij}\right)^2, \qquad \mathcal{M}_i = \{j : m_{ij} = 1\}, \quad \hat{\bx}_i = g_\psi(f_\psi(\tilde{\bx}_i)) \label{eq:mae}
\end{equation}

By predicting masked features from visible ones, the encoder learns the correlational structure of the clinical feature space without supervision.
\end{mathbox}

Training: 300 epochs, AdamW ($\eta = 10^{-3}$, weight decay $10^{-4}$). After pretraining, $f_\psi$ is frozen and used as a feature extractor.

\subsection{Classifier with Monte Carlo Dropout}

The downstream classifier applies the frozen encoder followed by a stochastic classification head:

\begin{mathbox}[Definition 7: MC Dropout for Epistemic Uncertainty Estimation]
\textbf{Forward pass:}
\begin{equation}
\hat{y}_i = \sigma\!\left(W_2\;\text{ReLU}(W_1 f_\psi(\bx_i) + b_1) + b_2\right) \label{eq:classifier}
\end{equation}
with Dropout$(p{=}0.3)$ applied to each hidden layer during both training and inference.

\textbf{Loss function} (class-weighted binary cross-entropy):
\begin{equation}
\mathcal{L}_{\text{BCE}} = -\frac{1}{N}\sum_{i=1}^{N}\left[w_+ y_i \log\hat{y}_i + (1-y_i)\log(1-\hat{y}_i)\right], \qquad w_+ = \frac{N_0}{N_1} \approx 84 \label{eq:bce}
\end{equation}

\textbf{MC Dropout inference:} At test time, dropout remains active. Run $T = 50$ stochastic forward passes for each subject:
\begin{equation}
\bar{p}_i = \frac{1}{T}\sum_{t=1}^{T}\hat{y}_i^{(t)}, \qquad \hat{\sigma}_i^{\text{epistemic}} = \sqrt{\frac{1}{T}\sum_{t=1}^{T}\left(\hat{y}_i^{(t)} - \bar{p}_i\right)^2} \label{eq:mc}
\end{equation}
$\bar{p}_i$ is the calibrated prediction; $\hat{\sigma}_i^{\text{epistemic}}$ captures model uncertainty (high $\hat{\sigma}$ = the model is unsure about patient $i$).
\end{mathbox}

\subsection{Temporal Modeling: Bidirectional GRU with Self-Attention}

Each subject's AE history is a variable-length sequence $(\bx_1^{(i)}, \ldots, \bx_{T_i}^{(i)})$ where each event is encoded as $\bx_t = [\text{AESTDY}_t / 365,\;\text{severity}_t / 3,\;\text{SOC\_code}_t / |\mathcal{S}|] \in \R^3$, deliberately excluding the seriousness flag to prevent target leakage.

\begin{mathbox}[Definition 8: Bidirectional GRU with Bahdanau Attention]
\textbf{GRU cell} (forward direction, hidden dimension $h = 32$):
\begin{align}
\mathbf{z}_t &= \sigma\!\left(W_z[\bh_{t-1},\,\bx_t] + b_z\right) & \text{(update gate)} \label{eq:gru_z}\\
\mathbf{r}_t &= \sigma\!\left(W_r[\bh_{t-1},\,\bx_t] + b_r\right) & \text{(reset gate)} \label{eq:gru_r}\\
\tilde{\bh}_t &= \tanh\!\left(W_h[\mathbf{r}_t \odot \bh_{t-1},\,\bx_t] + b_h\right) & \text{(candidate activation)} \label{eq:gru_h}\\
\bh_t &= (1 - \mathbf{z}_t) \odot \bh_{t-1} + \mathbf{z}_t \odot \tilde{\bh}_t & \text{(hidden state)} \label{eq:gru_out}
\end{align}
The backward GRU produces $\overleftarrow{\bh}_t$; concatenation yields $\bh_t^{\text{bi}} = [\overrightarrow{\bh}_t;\,\overleftarrow{\bh}_t] \in \R^{2h}$.

\textbf{Bahdanau (additive) attention:}
\begin{equation}
e_t = \mathbf{v}^\top\tanh\!\left(W_a\bh_t^{\text{bi}} + b_a\right), \qquad \alpha_t = \frac{\exp(e_t)}{\sum_{s=1}^{T}\exp(e_s)} \label{eq:attn}
\end{equation}
\begin{equation}
\mathbf{c} = \sum_{t=1}^{T}\alpha_t\,\bh_t^{\text{bi}} \label{eq:context}
\end{equation}
The context vector $\mathbf{c} \in \R^{2h}$ feeds a classification head. The attention weights $\balpha = (\alpha_1, \ldots, \alpha_T) \in \Delta^{T-1}$ are interpretable: they reveal \textit{which events in the temporal sequence the model considers most predictive of SAE}.
\end{mathbox}

\subsection{Integrated Gradients for Feature Attribution}

For a differentiable classifier $F: \R^d \to [0,1]$, we compute attributions using the path integral from a zero baseline $\bx' = \mathbf{0}$:

\begin{mathbox}[Definition 9: Integrated Gradients (Sundararajan et al., 2017)]
\begin{equation}
\text{IG}_j(\bx) = (x_j - x'_j) \times \int_{\alpha=0}^{1}\frac{\partial F(\bx' + \alpha(\bx - \bx'))}{\partial x_j}\,d\alpha \label{eq:ig}
\end{equation}

\textbf{Completeness axiom} (what makes IG principled --- attributions are exact, not approximate):
\begin{equation}
\sum_{j=1}^{d}\text{IG}_j(\bx) = F(\bx) - F(\bx') \label{eq:completeness}
\end{equation}

\textbf{Sensitivity axiom:} If $x_j \neq x'_j$ and $F$ depends on feature $j$, then $\text{IG}_j \neq 0$.

\textbf{Numerical approximation} (Riemann sum, $M = 200$ steps):
\begin{equation}
\text{IG}_j(\bx) \approx \frac{x_j - x'_j}{M}\sum_{m=1}^{M}\frac{\partial F\!\left(\bx' + \tfrac{m}{M}(\bx - \bx')\right)}{\partial x_j} \label{eq:ig_approx}
\end{equation}
\end{mathbox}

\subsection{Evaluation Protocol}

\begin{mathbox}[Definition 10: Cross-Validation and Bootstrap Confidence Intervals]
\textbf{Stratified $k$-fold CV} ($k = 5$): each fold preserves the class ratio $N_1/N \approx 0.012$. For fold $f$:
\begin{equation}
\text{AUROC}_f = \int_0^1 \text{TPR}_f(\tau)\;d\,\text{FPR}_f(\tau) \label{eq:auroc}
\end{equation}

\textbf{Bootstrap CI} ($B = 2000$ resamples of fold-level scores):
\begin{equation}
\text{CI}_{95\%} = \left[\hat{Q}_{0.025}\!\left(\{\text{AUROC}^{*b}\}_{b=1}^B\right),\;\;\hat{Q}_{0.975}\!\left(\{\text{AUROC}^{*b}\}_{b=1}^B\right)\right] \label{eq:bootstrap}
\end{equation}

Additional metrics --- Average Precision (AP) and Brier score:
\begin{equation}
\text{AP} = \sum_{k}\left(\text{Recall}_k - \text{Recall}_{k-1}\right)\text{Precision}_k, \qquad
\text{Brier} = \frac{1}{N}\sum_{i=1}^{N}(\hat{y}_i - y_i)^2 \label{eq:ap_brier}
\end{equation}
\end{mathbox}

% ══════════════════════════════════════════════════════════════════════
\section{Results}
% ══════════════════════════════════════════════════════════════════════

\subsection{Clinical Findings}

\begin{insightbox}[Clinical Story: What the Figures Tell the Medical Monitor]
\textbf{Fig~\ref{fig:volcano} (Volcano):} Pruritus ($p < 0.001$), application site pruritus, application site erythema, and dizziness are statistically significant in the High Dose arm. This is a \textbf{local dermatological tolerability} problem, not systemic toxicity.

\textbf{Fig~\ref{fig:km} (KM):} AE-free survival curves diverge by day 15. Median time-to-first-AE: $\sim$25 days (High Dose) vs $\sim$50 days (Placebo). \textbf{If a patient tolerates month 1, long-term tolerance is likely.}

\textbf{Fig~\ref{fig:dashboard} (Dashboard):} DSMB-ready composite: dose-dependent incidence, severity shift toward moderate/severe with dose, exposure-response $r = 0.93$.
\end{insightbox}

\begin{figure}[H]
\centering
\includegraphics[width=0.88\textwidth]{figures/fig01_volcano.png}
\caption{Adverse event disproportionality (volcano plot). Each bubble represents one AE preferred term; $x$-axis: $\log_2(\text{RR})$; $y$-axis: $-\log_{10}(p)$; bubble size $\propto$ total event count. Dashed line: $p = 0.05$ threshold.}
\label{fig:volcano}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.88\textwidth]{figures/fig03_kaplan_meier.png}
\caption{Time-to-first AE: Kaplan--Meier estimate $\hat{S}(t)$ with Greenwood 95\% CI bands and number-at-risk table. Clear dose-response separation from study day 15.}
\label{fig:km}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/fig06_safety_dashboard.png}
\caption{Composite clinical safety dashboard. (A) Top AE incidence with Wald CI. (B) Severity profile by arm. (C) SAE rate with Wilson score CI. (D) Exposure-response scatter ($r = 0.93$, $p < 0.001$).}
\label{fig:dashboard}
\end{figure}

\subsection{Deep Learning Results}

\begin{table}[H]
\centering\small
\caption{Complete ablation study. 5-fold stratified CV with bootstrap 95\% CI on AUROC.}
\label{tab:ablation}
\begin{tabular}{llc}
\toprule
\textbf{Model} & \textbf{Components} & \textbf{AUROC} \\
\midrule
Logistic Regression & $\ell_2$-regularized, $C=1.0$ & \textbf{0.690} \\
Random Forest & 100 trees, class\_weight=balanced & 0.348 \\
Gradient Boosting & 100 trees, max\_depth=3 & 0.358 \\
\midrule
MLP (scratch) & Random init, 100 epochs & 0.568 \\
MLP + pretraining & + frozen MAE encoder ($f_\psi$) & 0.635 \\
MLP + pretrain + aug & + CVAE synthetic minorities & 0.656 \\
MLP + aug only & CVAE aug, no pretraining & 0.928$^*$ \\
Full pipeline + MC & All + 50-pass MC Dropout & 0.763 \\
\bottomrule
\multicolumn{3}{l}{\scriptsize $^*$Suspected augmentation overfitting --- see Section~\ref{sec:limitations}.}
\end{tabular}
\end{table}

\textbf{Ablation decomposition:} Pretraining adds $\Delta\text{AUROC} = +0.067$ (from 0.568 to 0.635). CVAE augmentation adds $+0.021$ incrementally (from 0.635 to 0.656). The combined effect ($+0.088$) is sub-additive, suggesting partial overlap in learned representations.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/fig07_ml_dashboard.png}
\caption{ML evaluation dashboard. (A) AUROC forest plot with bootstrap CI for all 8 models. (B) Ablation: incremental $\Delta$AUROC from pretraining and augmentation. (C) Calibration curve (GBM vs best neural). (D) MC Dropout epistemic uncertainty distribution. (E) Learning curve: AUROC vs training $N$. (F) CVAE latent space ($t$-SNE) with SAE subjects highlighted.}
\label{fig:ml}
\end{figure}

\subsection{Feature Attribution Results}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/fig09_integrated_gradients.png}
\caption{Integrated Gradients attribution. (A) Global feature importance: $\E_i[|\text{IG}_j(\bx_i)|]$ across all subjects. (B) Per-subject beeswarm plot: each dot is one subject, color encodes standardized feature value, $x$-position encodes signed attribution $\text{IG}_j(\bx_i)$.}
\label{fig:ig}
\end{figure}

\begin{insightbox}[Novel Clinical Insight from Neural Attribution]
The strongest predictor of SAE is \textbf{diversity of non-serious AE types} ($\texttt{n\_unique\_ae}$, $\E[|\text{IG}|] = 0.78$) and their \textbf{temporal spread} ($\texttt{ae\_spread}$, $\E[|\text{IG}|] = 0.63$). Patients experiencing many \textit{different} types of mild AEs across different time points are at higher risk than those with repeated occurrences of the same event. This multivariate temporal pattern is invisible to standard univariate frequency tables.
\end{insightbox}

% ══════════════════════════════════════════════════════════════════════
\section{Limitations}\label{sec:limitations}
% ══════════════════════════════════════════════════════════════════════

\begin{warningbox}[Limitations and Caveats]
\begin{enumerate}
\item \textbf{Extreme class imbalance:} $N_1 = 3$ out of $N = 254$ (1.2\%). Statistical power analysis: detecting $\Delta\text{AUROC} = 0.10$ at 80\% power requires $N \approx 400$+ subjects with $\geq 20$ SAE events.
\item \textbf{Augmentation anomaly:} MLP + aug only achieves AUROC = 0.928 without pretraining. The CVAE likely generates synthetic positive subjects too similar to the real positives used within the same CV fold, inflating apparent performance. Leave-augmentation-out validation is needed.
\item \textbf{Tree ensemble failure:} Random Forest (0.348) and Gradient Boosting (0.358) perform below chance ($< 0.50$). At $N_0/N_1 \approx 84$, bagging and boosting cannot learn a decision boundary without sophisticated minority handling.
\item \textbf{GRU non-convergence:} Training loss remains flat at $\sim$1.37 across 150 epochs. Three positive sequences provides insufficient signal for temporal learning.
\item \textbf{Single trial, no external validation.} Generalizability to other compounds, indications, and trial designs is unknown.
\item \textbf{Not clinical evidence.} This is a methodological demonstration on a pilot dataset, not a clinical evaluation of Xanomeline safety or efficacy.
\end{enumerate}
\end{warningbox}

% ══════════════════════════════════════════════════════════════════════
\section{From Data Files to Clinical Decisions}
% ══════════════════════════════════════════════════════════════════════

\begin{scenariobox}[What the Medical Monitor Receives at the End of This Pipeline]
\begin{enumerate}
\item \textbf{Safety profile} (Figs~\ref{fig:volcano},~\ref{fig:dashboard}): dermatological tolerability issue, not systemic toxicity. Dose-dependent application site reactions, exposure-response $r = 0.93$.
\item \textbf{Temporal map} (Figs~2--4): critical window is weeks 1--4. Patients surviving month 1 will likely tolerate long-term treatment. Cardiac events peak later with different latency.
\item \textbf{Risk prediction} (Fig~\ref{fig:ml}): best honest model is $\ell_2$-logistic regression (AUROC = 0.690). Neural pipeline (0.656--0.763) with interpretable explanations.
\item \textbf{Per-patient explanations} (Fig~\ref{fig:ig}): Integrated Gradients attribution is leakage-free, clinically interpretable, and satisfies the completeness axiom (Eq.~\ref{eq:completeness}).
\item \textbf{Honest uncertainty} (Fig~\ref{fig:ml}D): MC Dropout $\hat{\sigma}^{\text{epistemic}}$ (Eq.~\ref{eq:mc}) is appropriately higher for SAE predictions than non-SAE.
\end{enumerate}
\end{scenariobox}

% ══════════════════════════════════════════════════════════════════════
\section{Conclusion}
% ══════════════════════════════════════════════════════════════════════

ClinOps AI demonstrates that modern deep learning can augment traditional pharmacovigilance on real clinical trial data. The visualization pipeline (Stages 1--4) delivers immediate clinical value: volcano plots for disproportionality, Kaplan--Meier for temporal patterns, and DSMB-ready composite dashboards. The ML pipeline (Stages 5--6) shows that self-supervised pretraining ($\Delta = +0.067$) and CVAE augmentation ($\Delta = +0.021$) each contribute measurably under controlled ablation, and that Integrated Gradients provide per-subject explanations satisfying the completeness axiom. At $N = 254$ with 3 SAE events, $\ell_2$-regularized logistic regression (AUROC = 0.690) remains the most reliable single predictor.

The most transferable contribution is methodological: the identification and correction of a leakage bug ($\texttt{n\_sae}$ as both feature and target), a reminder that in clinical ML, where outcome variables and input features share deep causal structure, systematic leakage auditing is not a best practice --- it is a prerequisite.

\subsection*{Conflicts of Interest}
The author declares no conflict of interest.

\subsection*{Data Availability}
All data used in this project is publicly available from the PHUSE GitHub repository. The complete analytical pipeline, including notebook code and LaTeX source, is available in the project repository.

\begin{thebibliography}{99}
\bibitem{sundararajan2017} Sundararajan, M., Taly, A., \& Yan, Q. (2017). Axiomatic attribution for deep networks. \textit{Proceedings of the 34th International Conference on Machine Learning}, 3319--3328.
\bibitem{kingma2014} Kingma, D. P., \& Welling, M. (2014). Auto-encoding variational Bayes. \textit{Proceedings of the 2nd International Conference on Learning Representations}.
\bibitem{gal2016} Gal, Y., \& Ghahramani, Z. (2016). Dropout as a Bayesian approximation: Representing model uncertainty in deep learning. \textit{Proceedings of the 33rd International Conference on Machine Learning}, 1050--1059.
\bibitem{cho2014} Cho, K., van Merri\"{e}nboer, B., Gulcehre, C., et al. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. \textit{Proceedings of EMNLP}, 1724--1734.
\bibitem{bahdanau2015} Bahdanau, D., Cho, K., \& Bengio, Y. (2015). Neural machine translation by jointly learning to align and translate. \textit{Proceedings of the 3rd International Conference on Learning Representations}.
\bibitem{devlin2019} Devlin, J., Chang, M.-W., Lee, K., \& Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding. \textit{Proceedings of NAACL-HLT}, 4171--4186.
\bibitem{he2022} He, K., Chen, X., Xie, S., Li, Y., Doll\'{a}r, P., \& Girshick, R. (2022). Masked autoencoders are scalable vision learners. \textit{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 16000--16009.
\bibitem{kaplan1958} Kaplan, E. L., \& Meier, P. (1958). Nonparametric estimation from incomplete observations. \textit{Journal of the American Statistical Association}, 53(282), 457--481.
\bibitem{cdisc2021} CDISC (2021). Study Data Tabulation Model (SDTM) Implementation Guide v3.4. \textit{Clinical Data Interchange Standards Consortium}.
\bibitem{iche9} ICH (1998). ICH E9: Statistical principles for clinical trials. \textit{International Conference on Harmonisation}.
\end{thebibliography}

\end{document}
